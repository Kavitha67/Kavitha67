{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kavitha67/Kavitha67/blob/main/Project_De_duplicate_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                        # START\n",
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ],
      "metadata": {
        "id": "7eK2w2sj_oK7",
        "outputId": "5da5b9bb-3ccb-49f6-d133-f637dfcf9b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=9ab33bcae9e5483d971710c21b9c0b11a3bf3d28104b96cb71561c59a4ceb30f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/21/84/970b03913d0d6a96ef51c34c878add0de9e4ecbb7c764ea21f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install delta-spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-ecYlwqJaqB",
        "outputId": "386aece1-dab7-40de-c13c-6089a8fb4644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-2.2.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from delta-spark) (6.0.0)\n",
            "Collecting pyspark<3.4.0,>=3.3.0\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.12.1)\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=579cab964bbd051e7ca81ba0d92d73eeb54582e82d2576e124ca3a45137e8756\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, delta-spark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9\n",
            "    Uninstalling py4j-0.10.9:\n",
            "      Successfully uninstalled py4j-0.10.9\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.0.1\n",
            "    Uninstalling pyspark-3.0.1:\n",
            "      Successfully uninstalled pyspark-3.0.1\n",
            "Successfully installed delta-spark-2.2.0 py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from delta import *\n",
        "from delta.tables import *\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")"
      ],
      "metadata": {
        "id": "jzouvZf__rkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
        ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
        ".config(\"spark.jars.packages\",\"io.delta:delta-core_2.12:2.0.0\") \n",
        "\n",
        "spark=configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "fFKrmVWDJhvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= spark.read.csv(\"/content/Book1.csv\", header= True)\n",
        "df2= spark.read.csv(\"/content/Book3.csv\", header= True)"
      ],
      "metadata": {
        "id": "pmsy5DvOJlGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df1.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\")\\\n",
        "                         .option(\"path\", \"Delta1\").saveAsTable(\"Delta__1\")\n",
        "df4 = df2.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\")\\\n",
        "                         .option(\"path\", \"Delta2\").saveAsTable(\"Delta__2\")\n",
        "                         # END"
      ],
      "metadata": {
        "id": "IECGP3L4JpdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6=spark.read.format(\"delta\").load(\"/content/spark-warehouse/Delta1\", inferSchema= True)\n",
        "df7= spark.read.format(\"delta\").load(\"/content/spark-warehouse/Delta2\",inferSchema= True)"
      ],
      "metadata": {
        "id": "R_vuIZOfRvBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df6.show()\n",
        "df6.printSchema()\n",
        "#df7.show()"
      ],
      "metadata": {
        "id": "TP2aZcLwJt2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab0448c-f378-4fee-be7b-845fbea6bf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- NAME: string (nullable = true)\n",
            " |-- AGE: string (nullable = true)\n",
            " |-- SALARY: string (nullable = true)\n",
            " |-- DEPT: string (nullable = true)\n",
            " |-- EXPERIENCE: string (nullable = true)\n",
            " |-- JOINING : string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName('PySpark_Tutorial').getOrCreate() "
      ],
      "metadata": {
        "id": "CeagOALs_vPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") "
      ],
      "metadata": {
        "id": "nSMT5SP-oe4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions  as f\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from delta import * \n",
        "import datetime\n"
      ],
      "metadata": {
        "id": "etdWyA6qQ0sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DE-DUPLICATE PROCESS:"
      ],
      "metadata": {
        "id": "9chqdWxl_1yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from delta import *\n",
        "from delta.tables import *\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")"
      ],
      "metadata": {
        "id": "R5hGImzcIN3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
        ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
        ".config(\"spark.jars.packages\",\"io.delta:delta-core_2.12:2.0.0\")\n",
        "\n",
        "spark=configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "r0PBWOhyIR__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SCDTypeII:\n",
        "   def __init__(self,business_key_columns: str,\n",
        "              modification_date_time_column:str,\n",
        "              source_path:str,\n",
        "              update_dataframe: DataFrame =None,\n",
        "              provided_file_path_flag:bool=False,\n",
        "              is_historical: bool = False,\n",
        "              capture_delete_flag:bool=False,\n",
        "              modification_date_time_logic:str=None\n",
        "               ):\n",
        "    \n",
        "       \n",
        "    \n",
        "     self.business_key_columns = business_key_columns.split(',')\n",
        "     self.modificatiob_date_time_columns=modification_date_time_column\n",
        "     self.source_path=source_path\n",
        "     if('--input_file_name' not in update_dataframe.columns):\n",
        "        self.update_dataframe=update_dataframe.withColumn('__input_file_name',lit(None).cast(StringType()))\n",
        "        self.update_dataframe.show()\n",
        "     else:\n",
        "        self.update.dataframe=update_dataframe.show()\n",
        "     #if_input_file_name\n",
        "     self.content_key_columns = sorted([column_name for column_name in self.update_dataframe.columns if column_name != '__input_file_name'],key=lambda element : element.lower())\n",
        "     self.provided_file_path_flag=provided_file_path_flag\n",
        "     self.capture_delete_flag=capture_delete_flag\n",
        "     self.is_historical=is_historical\n",
        "     self.modification_date_time_logic=modification_date_time_logic\n",
        "     source_path= \"/content/spark-warehouse/Delta1\"\n",
        "     #source_path= \"/content/Book3.csv\"\n",
        "     try:\n",
        "       self.source_dataframe=spark.read.format('delta').load(self.source_path)\n",
        "       source_path= \"/content/spark-warehouse/Delta1\"\n",
        "      \n",
        "     except Exception as ex:\n",
        "        if(\"is not a Delta Table.\" in str(ex)):\n",
        "          default_schema=self.update_dataframe.drop('__input_file_name').schema\n",
        "          self.source_dataframe= (spark.createDataFrame([],schema=default_schema)\n",
        "                           .withColumn('__current',lit(None).cast('bollean'))\n",
        "                           .withColumn('__activation_datetime',lit(None).cast('timestamp'))\n",
        "                           .withColumn('__deactivation_datetime',lit(None).cast('timestamp'))\n",
        "                           .withColumn('__business_key_hash',lit(None).cast('string'))\n",
        "                           .withColumn('__content_key_hash',lit(None).cast('string'))\n",
        "                           .withColumn('__is_historical',lit(None).cast('boolean'))\n",
        "                           )\n",
        "              "
      ],
      "metadata": {
        "id": "i8TKTQOhAhwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= SCDTypeII('Delta1.TRAINING' ,'df7.NAME','/content/spark-warehouse/Delta2',df7,True,True,True)\n",
        "print(data)\n",
        "\n",
        "\n",
        "\n",
        "               \n",
        "\n",
        "\"\"\"\n",
        "      business_key_columns: list of business keys \n",
        "      content_key_columns: list of content keys columns \n",
        "      modification_date_time_column: column name for the modification datetime column\n",
        "      source_path: the path to the initial delta table that we're aiming to update\n",
        "      update_dataframe: dataframe that would be updating the source delta table\n",
        "      provided_file_path_flag: determines whether the file path for the row content is provided in the dataframe\n",
        "      modification_date_time_logic: if modification_date_time_column doesn't exist or is not enough for sorting records, this string value can represent a logic that generates a column\n",
        "      of TimestampType.\n",
        "      capture_delete_flag: determines whether this delta table tracks deleted records \n",
        "      \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9jGTb_M__g9y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "605be44b-a81f-4778-ea75-f3872b033181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---+------+-------------+----------+----------+-------------------+-----------------+\n",
            "| ID|     NAME|AGE|SALARY|         DEPT|EXPERIENCE|  JOINING |__business_key_hash|__input_file_name|\n",
            "+---+---------+---+------+-------------+----------+----------+-------------------+-----------------+\n",
            "|  1|     kavi| 23| 10000|Data engineer|         2|26-08-2021|                 23|             null|\n",
            "|  2|     suba| 23| 23000| data science|         2|26-02-2022|                 23|             null|\n",
            "|  3|     hema| 65|129000|    analytics|         9|23-04-2022|                 65|             null|\n",
            "|  4|    surya| 34|112200|       devops|         2|12-08-2020|                 34|             null|\n",
            "|  5|    kavya| 34|  1200|data engineer|         4|23-04-2020|                 34|             null|\n",
            "|  6|      anu| 34|129000|    operation|         3|01-10-2022|                 34|             null|\n",
            "|  7|      jon| 22|  1344|     services|         1|11-09-2022|                 22|             null|\n",
            "|  8|     mosh| 45| 47000|     services|         7|10-02-2021|                 45|             null|\n",
            "|  9|jayakumar| 23| 15000|   operation |         3|11-03-2022|                 23|             null|\n",
            "| 10|  monisha| 23| 16000|     big data|         4|22-04-2023|                 23|             null|\n",
            "+---+---------+---+------+-------------+----------+----------+-------------------+-----------------+\n",
            "\n",
            "<__main__.SCDTypeII object at 0x7f1433c09760>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n      business_key_columns: list of business keys \\n      content_key_columns: list of content keys columns \\n      modification_date_time_column: column name for the modification datetime column\\n      source_path: the path to the initial delta table that we're aiming to update\\n      update_dataframe: dataframe that would be updating the source delta table\\n      provided_file_path_flag: determines whether the file path for the row content is provided in the dataframe\\n      modification_date_time_logic: if modification_date_time_column doesn't exist or is not enough for sorting records, this string value can represent a logic that generates a column\\n      of TimestampType.\\n      capture_delete_flag: determines whether this delta table tracks deleted records \\n      \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "update_dataframe= spark.read.format('delta').load(\"/content/spark-warehouse/Delta2\")\n",
        "source_dataframe=spark.read.format('delta').load(\"/content/spark-warehouse/Delta1\")"
      ],
      "metadata": {
        "id": "_6Hxewvq8_UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _schema_evolution_handler(self):\n",
        "    update_dataframe_data_columns = [column_name for column_name in self.update_dataframe.columns if not column_name.startswith('__')]\n",
        "    source_dataframe_data_columns = [column_name for column_name in self.source_dataframe.columns if not column_name.startswith('__')]\n",
        "    extra_columns_set = set(update_dataframe_data_columns) - set(source_dataframe_data_columns)\n",
        "    if(len(extra_columns_set) != 0): \n",
        "      #Means there are columns in updating dataframe that are not in source dataframe \n",
        "      #So we need to add the column to the silver raw table\n",
        "      print('Schema Evolution In Progress')\n",
        "      \n",
        "      #Adding extra columns source dataframe\n",
        "      for column_name in extra_columns_set:\n",
        "        self.source_dataframe = self.source_dataframe.withColumn(column_name, lit(None).cast(self.update_dataframe.schema[column_name].dataType))\n",
        "\n",
        "      #using self.content_key_columns to arrange self.source_dataframe columns to make sure __content_key_hash is generated consistently from order of columns perspective.\n",
        "      audit_columns = [column_name for column_name in self.source_dataframe.columns if column_name.startswith('__')]\n",
        "      self.source_dataframe = (self.source_dataframe\n",
        "                               .withColumn('__content_key_hash', sha2(concat_ws('|', *[col(column).cast(StringType()) for column in self.content_key_columns]), 256))\n",
        "                               .select(self.content_key_columns + audit_columns)\n",
        "                              )\n",
        "    \n",
        "      #overwriting silver raw table\n",
        "      self.source_dataframe.write.mode('overwrite').option(\"mergeSchema\", True).format('delta').save(self.source_path)\n",
        "     \n",
        "    return True\n",
        "print('done')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPxU0dQrVvXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c96a50-07fe-4d22-f1eb-3e89806443f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s= _schema_evolution_handler(extra_columns_set) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ws2aCCRt_Euj",
        "outputId": "1ae9b9bf-fae8-4ac1-d25c-2eb393844170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-447d1c524526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0m_schema_evolution_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mupdate_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _add_helper_columns_to_update(self):\n",
        "    \"\"\" Stage one preparation on current table before merge. Adding SCD_COLS to other herlper functions to aid table preparations. \n",
        "    Deduplicates table on content key hash where duplicates exist within the same '__bronze_landing_date_time'.\n",
        "    Also identifies which record should have the current flag enabled.\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: current dataframe with helper functions added.\n",
        "    \"\"\"\n",
        "    data_columns = self.content_key_columns + ['__current', '__activation_datetime', '__deactivation_datetime', '__minimum_activation_date', '__business_key_hash', '__content_key_hash', '__is_historical']\n",
        "    if(self.provided_file_path_flag):\n",
        "      self.initial_preparation = (self.update_dataframe\n",
        "                                  .withColumn('__input_file_name', col('__input_file_name'))\n",
        "                                  .withColumn('__bronze_landing_date_time', to_timestamp(regexp_replace(regexp_extract(col('__input_file_name'), '([0-9]{8}[/_][0-9]{6})', 1), '_', '/'), 'yyyyMMdd/HHmmss'))\n",
        "                                  .withColumn('__business_key_hash', sha2(concat_ws('|', *self.business_key_columns), 256))\n",
        "                                  .withColumn('__content_key_hash', sha2(concat_ws('|', *[col(column).cast(StringType()) for column in self.content_key_columns]), 256))\n",
        "                                  # take the first non-None value in [self.modification_date_time_logic, self.modification_date_time_column] and use it to generate __modification_date_time\n",
        "                                  .withColumn('__modification_date_time', expr(next(item for item in [self.modification_date_time_logic, self.modification_date_time_column] if item is not None)))\n",
        "                                 )\n",
        "    else:\n",
        "      self.initial_preparation = (self.update_dataframe\n",
        "                                  .withColumn('__input_file_name', input_file_name())\n",
        "                                  .withColumn('__bronze_landing_date_time', to_timestamp(regexp_replace(regexp_extract(col('__input_file_name'), '([0-9]{8}[/_][0-9]{6})', 1), '_', '/'), 'yyyyMMdd/HHmmss'))\n",
        "                                  .withColumn('__business_key_hash', sha2(concat_ws('|', *self.business_key_columns), 256))\n",
        "                                  .withColumn('__content_key_hash', sha2(concat_ws('|', *[col(column).cast(StringType()) for column in self.content_key_columns]), 256))\n",
        "                                  # take the first non-None value in [self.modification_date_time_logic, self.modification_date_time_column] and use it to generate __modification_date_time\n",
        "                                  .withColumn('__modification_date_time', expr(next(item for item in [self.modification_date_time_logic, self.modification_date_time_column] if item is not None)))\n",
        "                                 )\n",
        "    deduplication_window = Window.partitionBy('__content_key_hash').orderBy(col('__bronze_landing_date_time').desc())\n",
        "    current_window_spec = Window.partitionBy('__business_key_hash').orderBy(col('__modification_date_time').desc())\n",
        "    bkh_window_spec = Window.partitionBy('__business_key_hash')\n",
        "    self.update_dataframe = (self.initial_preparation\n",
        "                             # Isolate new records i.e. new content not new business keys \n",
        "                             .join(self.source_dataframe.alias(\"original\").where(col('__current')), '__content_key_hash', 'left_anti')\n",
        "                             .withColumn('__bronze_landing_date_time_row_number', row_number().over(deduplication_window))\n",
        "                             .where(col('__bronze_landing_date_time_row_number') == 1)\n",
        "                             .withColumn('__activation_datetime', col('__modification_date_time'))\n",
        "                             .withColumn('__deactivation_datetime', lag(col('__modification_date_time'), 1).over(current_window_spec))\n",
        "                             .withColumn('__modified_date_time_row_number', row_number().over(current_window_spec))                \n",
        "                             .withColumn('__current', when(col('__modified_date_time_row_number') == 1, lit(True)).otherwise(lit(False)))\n",
        "                             .withColumn('__minimum_activation_date', f.min(col('__modification_date_time')).over(bkh_window_spec))\n",
        "                             .withColumn('__is_historical', f.lit(self.is_historical))\n",
        "                             .select(*data_columns)\n",
        "                             .cache())\n",
        "    print(\"number of new record contents: \" + str(self.update_dataframe.count()))\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "vu0jP3zb-twq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _scd_type_II_merge_setup(self):\n",
        "\n",
        "    \"\"\" Stage the update by unioning two sets of records. \n",
        "    1. New records (records with new business keys) to be INSERTED and records that would be used to UPDATE currently existing records aka UPSERT records\n",
        "    2. Records that would UPDATE the currently existing records (records whose business keys exist in our source table)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: Dataframe ready for merge operation.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    self._schema_evolution_handler()\n",
        "    self._add_helper_columns_to_update()\n",
        "    staged_update = (self.update_dataframe.alias(\"update\")\n",
        "                     .withColumn('__mergeKey', lit(None))\n",
        "                     .unionByName(self.update_dataframe.alias(\"update\")\n",
        "                                  .join(self.source_dataframe.alias(\"original\").where(col('__current')), '__business_key_hash')\n",
        "                                  .withColumn('__mergeKey', when(col('update.__current') & (f.col('original.__content_key_hash') != f.col('update.__content_key_hash')), \n",
        "                                                                 col('__business_key_hash')))\n",
        "                                  .where(col('__mergeKey').isNotNull())\n",
        "                                  .select('update.*', '__mergeKey')))\n",
        "    return staged_update\n",
        "\n"
      ],
      "metadata": {
        "id": "z7xfhakm-RzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scd_type_II_merge(self):\n",
        "    \"\"\" Apply SCD Type 2 operation using merge. \n",
        "    1. Inserts the new records with its current flag set to true\n",
        "    2. Updates the previous current row to set current to false, and update the '__deactivation_datetime' from null to the '__activation_datetime' from the source.\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: Dataframe ready for merge operation.\n",
        "    \"\"\"\n",
        "    staged_update = self._scd_type_II_merge_setup()\n",
        "    source_dataframe = DeltaTable.forPath(spark, self.source_path)\n",
        "    (source_dataframe.alias(\"original\")\n",
        "     .merge(source = staged_update.alias(\"update\"), \n",
        "            condition = expr(\"original.__business_key_hash = update.__mergeKey\"))\n",
        "     .whenMatchedUpdate(condition = expr(\"original.__current = true AND original.__content_key_hash <> update.__content_key_hash\"), \n",
        "                        set = {'__current' : lit(False),\n",
        "                               '__deactivation_datetime' : col(\"update.__minimum_activation_date\") })\n",
        "     .whenNotMatchedInsertAll()\n",
        "     .execute())\n",
        "    if(self.capture_delete_flag):\n",
        "      source_dataframe = DeltaTable.forPath(spark, self.source_path)\n",
        "      remaining_business_keys = (self.initial_preparation.select('__business_key_hash', '__bronze_landing_date_time')\n",
        "                                 .withColumn('__max_bronze_landing_date_time', max(col('__bronze_landing_date_time')).over(Window.partitionBy()))\n",
        "                                 .where(col('__max_bronze_landing_date_time') == col('__bronze_landing_date_time'))\n",
        "                                 .select('__business_key_hash')\n",
        "                                )\n",
        "      self.deleted_records = (source_dataframe.toDF().where((col('__current')) & (~col('__is_historical'))).select('__business_key_hash')\n",
        "                              .join(remaining_business_keys, '__business_key_hash', 'left_anti')\n",
        "                             )\n",
        "      (source_dataframe.alias('original')\n",
        "       .merge(source = self.deleted_records.alias('deleted'),\n",
        "              condition = expr(\"original.__business_key_hash = deleted.__business_key_hash\")\n",
        "             )\n",
        "       .whenMatchedUpdate(condition = expr(\"original.__current = true\"),\n",
        "                          set = {'__current': lit(False),\n",
        "                                 '__deactivation_datetime': lit(datetime.datetime.now())})\n",
        "       .execute())\n",
        "    return True\n",
        "\n"
      ],
      "metadata": {
        "id": "4I_hNWBtsLsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_delta_change_data_feed_log(self, \n",
        "                                     versions: list = None):\n",
        "    \"\"\" View counts on change events after merge operation. This function should be called after the merge operation.\n",
        "    Returns information about the number of updated rows, number of deleted rows, number of inserted rows and number of affected rows.\n",
        "    \n",
        "    Args:\n",
        "        version: delta table version\n",
        "        \n",
        "    Returns: change feed dataframe containing quantitative data about merge operation\n",
        "    \"\"\"\n",
        "    if(versions is None):\n",
        "      delta = DeltaTable.forPath(spark, self.source_path)\n",
        "      delta_version = (delta\n",
        "                       .history()\n",
        "                       .where(col('operation') == 'MERGE')\n",
        "                       .orderBy(col('version').desc())\n",
        "                      )\n",
        "      if(self.capture_delete_flag):\n",
        "        interesting_versions = [element.version for element in delta_version.collect()[0:2]]\n",
        "      else:\n",
        "        interesting_versions = [element.version for element in delta_version.collect()[0:1]]\n",
        "    else:\n",
        "      interesting_versions = versions\n",
        "    change_log = (delta\n",
        "                  .history()\n",
        "                  .where(col('version').isin(interesting_versions))\n",
        "                  .withColumn('version', col('version'))\n",
        "                  .withColumn('change_timestamp', col('timestamp'))\n",
        "                  .withColumn('operation', col('operation'))\n",
        "                  .withColumn('execution_time_in_ms', col('operationMetrics.executionTimeMs'))\n",
        "                  .withColumn('num_deleted_rows', col('operationMetrics.numTargetRowsDeleted'))\n",
        "                  .withColumn('num_updated_rows', col('operationMetrics.numTargetRowsUpdated'))\n",
        "                  .withColumn('num_inserted_rows', col('operationMetrics.numTargetRowsInserted'))\n",
        "                  .select('version', 'change_timestamp', 'operation', 'execution_time_in_ms', 'num_deleted_rows', 'num_updated_rows', 'num_inserted_rows'))\n",
        "    return change_log"
      ],
      "metadata": {
        "id": "l8aCX7BD78NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCDTypeII\n",
        " _schema_evolution_handler\n",
        "_add_helper_columns_to_update\n",
        "_scd_type_II_merge_setup\n",
        "scd_type_II_merge\n",
        "get_delta_change_data_feed_log"
      ],
      "metadata": {
        "id": "dKXyTiamH5Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "F6KVz9zd5fTJ",
        "outputId": "ad5865d3-dc67-4946-a273-779db7ba04ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b6006f9-ba86-4c50-9ddf-f08e6eb1ba12\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b6006f9-ba86-4c50-9ddf-f08e6eb1ba12\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Book1.csv to Book1 (1).csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Book1.csv': b'ID,NAME,AGE,SALARY,DEPT,EXPERIENCE\\r\\n1,kavi,23,10000,Data engineer,2\\r\\n2,suba,23,23000,data science,2\\r\\n3,hema,65,129000,analytics,9\\r\\n4,surya,34,112200,devops,2\\r\\n5,kavya,34,1200,data engineer,4\\r\\n6,anu,34,129000,operation,3\\r\\n7,jon,22,1344,services,1\\r\\n8,mosh,45,47000,services,7\\r\\n9,jayakumar,23,15000,operation ,3\\r\\n10,monisha,23,16000,big data,4\\r\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parquet Ingestion:\n",
        "data_read_string= file_read_regular_expression_generator(bronze_storage_account,bronze_container,bronze_path,process_start_datetime,process_end_datetime)\n",
        "if(data_read_string.endswith('{}') and bypass_missing_failure \n",
        "   ):\n",
        "  dbutils.datetime= spark.read.parquet(date_read_string)\n",
        "\n",
        "  scd_update= SCDTypeII(business_key_columns=business_key_columns,\n",
        "                        modification_date_time_column= modification_date_time_column,\n",
        "                        source_path= \"tmp\".format(silver_raw_container,silver_storage_account,silver_raw_path),\n",
        "                        update_dataframe=parquet_dataframe,\n",
        "                        provided_file_path_flag= False,\n",
        "                        is_historical= is_historical,\n",
        "                        capture_delete_flag=capture_delete_flag,\n",
        "  )\n",
        "\n",
        "  scd_update.scd_type_II_merge()"
      ],
      "metadata": {
        "id": "DQqr2c316J7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfHF7XLvuuPe",
        "outputId": "5953f80c-9c71-4f4b-d0a3-a7a270017e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.2.2\n",
            "  Using cached pyspark-3.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark==3.2.2) (0.10.9.5)\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.3.1\n",
            "    Uninstalling pyspark-3.3.1:\n",
            "      Successfully uninstalled pyspark-3.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "delta-spark 2.2.0 requires pyspark<3.4.0,>=3.3.0, but you have pyspark 3.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R1bBUz4Pu6iI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}